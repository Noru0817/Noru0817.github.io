[ { "title": "마이크로서비스 & 도커(Docker) 🐳", "url": "/posts/Docker/", "categories": "Docker, Terms", "tags": "단기 계획, Microservice, Kubernetes, Docker", "date": "2024-09-09 18:52:45 +0900", "snippet": "안녕하세요! 오늘은 마이크로서비스와 Docker에 대하여 간단하게 용어 설명해드리려고 합니다.최근 들어 사용자에게 제공되는 서비스들은 주로 ‘마이크로서비스 아키텍처(Microservice Architecture)’라는 특성으로 제공됩니다. 마이크로서비스에 대해 설명하기에 앞서, 기존에 있던 방식인 모놀리틱(Monolithic) 방식에 대해 설명드려야겠죠?모놀리틱 구조란 기존의 전통적인 웹 시스템 개발 스타일로, 하나의 애플리케이션 내에 모든 로직들이 들어가있는 방식을 이야기합니다. 모놀리틱 구조의 특징은 다음과 같습니다.모놀리틱 구조 특징 call-by-reference : 각 컴포넌트들은 상호 호출 함수를 이용하는 구조 간편한 개발 및 배포 : 전체 애플리케이션을 개발툴 등을 이용하여 하나의 애플리케이션으로 개발, 배포, 테스트 진행→ 이에 따라 규모가 작은 애플리케이션은 배포 및 운영 관리가 용이하지만 긴 빌드 및 배포 시간, 협업의 어려움, 구조 및 특성의 이해 어려움 등 하나의 애플리케이션이 대용량 서비스로 전환되는 경우 발생하는 문제점이 너무 많기 때문에, 이렇게 많은 요소들을 최소한의 단위로 나누어 개발하는 것이 바람직할 것이라는 아이디어에서 시작된 것이 바로 마이크로서비스입니다.마이크로서비스 구조 (Microservice Architecture)애플리케이션을 느슨하게 결합된 서비스의 모임으로 구조화하는 서비스 지향 아키텍쳐(Service Oriented Architecture, 이하 SOA)의 일종인 소프트웨어 개발 스타일입니다. 마이크로서비스의 특징은 다음과 같습니다.마이크로서비스 특징 서비스 분리 : 각 기능을 독립적인 서비스로 분리하여 개발 및 배포 독립적 배포 : 각 서비스를 독립적으로 배포할 수 있어 유연성이 높음 기술 다양성 : 각 서비스에 적합한 기술 스택 선택 가능 API 통신 : 서비스간 통신이 주로 API를 통해 진행 확장성 : 필요한 서비스만 선택적으로 확장 가능 장애 격리 : 한 서비스의 문제가 전체 시스템에 미치는 영향을 최소화 팀 자율성 : 각 서비스를 담당하는 팀을 독립적으로 운영 가능이렇게 모든 서비스를 최소한의 단위로 나누어 개발하다 보니, 각 서비스에 대한 독립적인 환경이 조성되고, 여기서 파생되는 개발, 배포 등에서의 편의성이 많이 보장되는 구조가 형성되기 때문에 아직까진 ‘마이크로서비스는 정확히 이런 방식으로 운영된다’라는 정확한 정의는 없지만, 많은 회사들이 마이크로서비스 구조를 채택하려 하고 있고, 이미 채택하고 있기도 합니다. (대표적인 예시: 구글의 ‘보그(Borg)’)마이크로서비스 구현을 위해서는 ‘컨테이너(Container)’라는 개념이 수반됩니다. 컨테이너란 애플리케이션과 그 실행에 필요한 모든 종속성(라이브러리, 바이너리, 설정 파일 등)을 하나의 패키지로 묶은 소프트웨어 유닛입니다. Linux를 기준으로 컨테이너를 OS 상에서 사용하기 위해선 기존에는 도커를 이용하여 Dockerfile을 생성한 뒤, 이 파일을 이미지로 빌드(build)하고, 도커를 통해 해당 이미지를 실행시키면 비로소 해당 이미지가 컨테이너가 되어 애플리케이션이 실행되는 방식이었습니다.그러나, 2021년 8월에 쿠버네티스 v1.22가 배포된 이후로 쿠버네티스에서의 도커 지원은 중단되었습니다. 정확히는 도커의 컨테이너 런타임을 사용할 수 없게 되었는데, 그 이유는 쿠버네티스를 이용한 여러 운영 방안들이 채택되면서 그에 맞는 컨테이너 런타임들이 생겨나게 되었는데, 이때 여러 컨테이너 런타임과 통신할 수 있도록 하는 ‘CRI(Container Runtime Interface)’라는 표준 인터페이스가 도입되었기 때문입니다.도커의 경우 CRI 탄생 이전에 만들어진 기술이었기 때문에, 이와 호환되지 않았고, 도커를 최대한 CRI에 맞춰 활용하고자 하드코딩하여 만든 도커와 CRI사이의 징검다리 역할인 ‘dockershim’도 만들어졌었지만, 쿠버네티스의 ‘kubelet’자체에 불필요한 복잡성을 초래하는 도커에 의존적인 코드들이 많이 만들어지면서 여러 컨테이너 런타임이 문제없이 돌아가게 하고자 한 쿠버네티스의 오픈소스 철학과 맞지 않아 결국 지원을 중단하게 되었습니다.(하지만 여전히 이미지를 빌드 및 관리하는 등, 도커 자체적인 기능은 사용중이기 때문에, 도커는 절대 배제하면 안되는 기술입니다 😤)따라서 현재 컨테이너를 쿠버네티스 환경에서 사용할 땐 다음과 같은 과정을 거치게 됩니다.graph TD subgraph \"쿠버네티스 클러스터\" A[쿠버네티스 마스터 노드] --&gt; B[쿠버네티스 워커 노드] A --&gt; C[쿠버네티스 워커 노드] end subgraph \"워커 노드\" B --&gt; D[kubelet] D --&gt; E[CRI] E --&gt; F[containerd] F --&gt; G[runc] G --&gt; H[컨테이너] G --&gt; I[컨테이너] end subgraph \"컨테이너 레지스트리\" J[이미지 저장소] end F -.-&gt; J클러스터 환경의 마스터 노드에 특정 애플리케이션 사용에 대한 요청이 들어오면, 이 작업은 워커 노드에게 전달되고, 해당 노드의 kubelet이 CRI와 통신하여 적절한 컨테이너 런타임을 선택하고, 컨테이너 런타임은 이미지 레지스트리에서 요청된 애플리케이션이 담긴 이미지를 가져와 runc를 이용하여 해당 이미지를 컨테이너로써 실행시키게 됩니다.컨테이너 가상화가 적용된 호스트 PC의 모식도는 다음과 같습니다.여기까지 마이크로서비스 및 도커에 대한 전반적인 설명이었습니다. 긴 글 읽어주셔서 감사합니다!!! 😌" }, { "title": "쿠버네티스 기반 학과 서버 설계", "url": "/posts/K8s-Architecture/", "categories": "Kubernetes, Server", "tags": "단기 계획, 용어 정리", "date": "2024-09-08 10:15:38 +0900", "snippet": "드디어, 실질적으로 서버 구축에 대한 이야기를 다루게 되었습니다 ㅎㅎ저를 비롯한 다른 연구생 분들도 정말 구하기 힘든 정보 모으고, 정제하느라 꽤 많은 시간을 보냈는데요(여름방학 2달 내내 했습니다…), 그 중에서 개인이 물리적 PC를 여러 대 가지고 있다면 시도해볼 수 있는 ‘클러스터 구축’에 대한 설계를 설명드리려고 합니다.오늘은 기본적인 용어부터 정의하고, 다음 글에 실질적 클러스터 구축에 대해 설명해볼게요!항상 실질적인 적용 이전에 용어 설명이나, 배경 설명으로 인해 답답하게 느끼시는 분들도 계실텐데, 매번 빨리 전달드리지 못하는 점 죄송합니다…ㅠㅠ아무래도 공학적인 관점에서 ‘같은 용어도 분야, 종사하고 있는 직장’에 따라 상이하게 사용되는 경향이 있는지라 누군가에게 설명을 하기에 앞서 용어에 대한 정의부터 하는게 바람직할 것 같아 이렇게 돌아가게 되는 것 같아요…저도 이 부분에 있어서 아직 미흡한 면이 있지만, 이왕 설명하게 되었으니, 최대한 헷갈리지 않도록 해보겠습니다!아! 이 글을 읽기에 앞서, 쿠버네티스는 기본적으로 ‘컨테이너화 된 애플리케이션’을 다루는 분야입니다. 컨테이너의 개념을 이해하기 위해선 도커(Docker)에서의 컨테이너 가상화에 대한 개념적 이해가 필요한데요, 이는 밑의 링크를 눌러보시면 확인하실 수 있습니다.https://noru0817.github.io/posts/Docker/클러스터클러스터란 ‘컨테이너화된 애플리케이션을 실행하기 위해 함께 작동하는 노드(물리적 또는 가상 머신)의 집합’입니다.쉽게 말씀드리자면 ‘내어줄 리소스를 가진 여러 컴퓨터들의 집합’입니다. 클러스터 내에는 다수의 노드들이 존재할 수 있지만, 크게 보면 두 종류의 노드만 존재합니다. 하나는 마스터 노드(Master Node), 나머지는 워커 노드(Worker Node)입니다.이름에서 눈치 채셨을 수도 있지만, 한 클러스터에 속한 모든 워커 노드의 관리는 마스터 노드가 담당합니다.노드노드란 ‘컨테이너화된 애플리케이션을 실행하는 물리적 또는 가상 머신’을 뜻합니다. 저희 학과 서버는 물리적 PC를 사용하여 클러스터를 구성하였지만, 다른 글들을 보면 쿠버네티스를 경험하는 자체에 의미를 두고 가상 머신 여러 개를 클러스터를 구성하는데 사용하기도 합니다.앞서 잠시 언급한 바와 같이 노드는 크게 ‘마스터 노드’와 ‘워커 노드’로 구분할 수 있습니다.마스터 노드는 모든 노드의 작업 및 상태를 관리하는 중앙 관리자 같은 역할이기 때문에 ‘컨트롤 플레인(Control-Plane)’이라고 불리기도 하는데요, 쿠버네티스 관련 문서를 찾아보니 요즘은 사용되는 용도 자체가 관리가 목적이다 보니 마스터 노드라는 표현 보다는 컨트롤 플레인이라는 표현을 더 많이 사용한다고 합니다.컨트롤 플레인컨트롤 플레인은 다음과 같은 구성요소를 가집니다. kube-apiserver: API 요청을 처리하는 프론트엔드 etcd: 클러스터 데이터를 저장하는 분산 key-value 저장소 kube-scheduler: 파드(Pod)를 노드에 할당하는 스케줄러 kube-controller-manager: 다양한 컨트롤러를 실행하여 클러스터 상태를 관리워커 노드워커 노드는 다음과 같은 구성요소를 가집니다. kubelet: 노드의 주요 에이전트로, 파드의 생성, 시작, 중지 관리 컨테이너 런타임: Docker와 같은 컨테이너 실행 소프트웨어 → 현재는 도커의 지원이 중단되어 Containerd 사용 kube-proxy: 네트워크 규칙 관리 → 파드 간 통신 가능다른 리소스들도 많지만, 아직 클러스터가 구축되지 않았기 때문에, 구축하는 글 이후에 워크로드에 대해 말씀드릴 때 같이 설명드릴 예정입니다. 읽어주셔서 감사합니다!!!" }, { "title": "AI 알고리즘 1주차", "url": "/posts/AI-algorithm-1st-week/", "categories": "3학년 2학기, AI 알고리즘", "tags": "단기 계획, 3학년 2학기, AI 알고리즘", "date": "2024-09-07 15:26:38 +0900", "snippet": "이번 글에선 AI 알고리즘 1주차 내용인 ‘이분법’에 대해 설명해드리려고 합니다.우리가 흔히 AI라고 부르는 기술을 만드는 과정은 딥러닝을 활용하여 최적의 해를 찾을 수 있는 정답에 가까운 방정식을 찾아가는 것입니다.이때 ‘정답에 가까운’이라는 표현이 중요한데요, 사용자가 정답을 찾길 바라는 어떤 문제에 대한 답은 어떠한 특정 방정식을 만족한다는 사실을 제외하고는 정보가 거의 없는 ‘수’일 때가 많습니다. (여기서 ‘입력값이 문자면?’이라는 의문점을 가지실수도 있는데, 실제로 모델을 만드는 과정에서 입력값이 문자일 경우 해당 문자를 수의 형태로 ‘인코딩(Encoding)’하여 알고리즘에서 사용합니다)따라서 정보가 부족했던 수를 마치 정보가 있는 것처럼, 즉 ‘정답은 아니지만 그와 유사한 형태’로 만듦으로써 예측할 수 있는 상태로 만드는 작업이 필요합니다. 그 방법 중 하나가 바로 ‘이분법’입니다.이분법은 함수의 구간을 정하고, 해당 구간의 중간값을 설정하여 ‘f(r) = 0’을 만족하는 실수 r값을 찾아가는 과정 또는 그 방법을 의미합니다.이때 조건 및 과정은 간략히 다음과 같습니다.이분법 함수 f가 어떤 구간에서 연속이며 구간의 양 끝점에서 서로 다른 부호의 함숫값을 가진다.→ ‘a ＜ b’ 일 때 ‘f(a)f(b) ＜ 0’ f는 구간 (a,b)에서 근을 가진다. 즉, 두 조건 ‘a ＜ r ＜ b’와 ‘f(r) = 0’을 만족하는 실수 r이 반드시 존재한다.이분법이라는 용어 자체가 어색하실 수 있는데요, 이 방식은 ‘사잇값 정리’를 기반으로 합니다.사잇값 정리‘a ≤ r ≤ b’ 인 폐구간에서 r이 움직일 때 f(r)은 f(a)와 f(b)사이의 구간을 빠짐없이 채우게 됩니다. 이때 r이 연속적이므로 f(r) 값 또한 연속적이게 되어 이산적인 형태는 존재하지 않습니다. 이러한 상황에서 f(a)와 f(b)는 부호가 서로 반대이기 때문에 함수 f는 a ≤ r ≤ b 어딘가에서 반드시 0이 되는 r값이 존재하게 됩니다.이분법은 이렇게 연속함수의 속성을 활용한 것인데요, 이를 알고리즘으로 표현할 수 있습니다.이분법 알고리즘 알고리즘의 각 단계에서 구간 [a, b]와 fa = f(a), fb = f(b)가 주어진다. 이때 fa, fb는 「fa * fb ＜ 0」을 만족한다. 위의 폐구간의 중점인 「c = 1/2(a + b)」를 정하고, fc = f(c)를 구한다. 우연히 f(c) = 0이라면 이 알고리즘의 목적은 달성된 것이다. 그러나, 대부분의 경우는 「fc ≠ 0」이고 이때 「fc * fa ＜ 0 또는 fc * fb ＜ 0」이다.주의할 점은 일반적으로 임의로 주어진 함수의 근을 찾는 과정에서 불필요한 함수의 계산을 피하는 것이 좋은데, 이는 해당 함수를 계산할 때 시간 복잡도 개념에서 ‘같은 결과를 도출하는 연산의 반복 작업’이 매 실행마다 포함될 수 있기 때문입니다. 따라서 이후에 필요한 함수 값은 미리 저장해놓는 ‘동적 프로그래밍(Dynamic Programming)’의 개념을 사용하면 위와 같은 문제를 예방할 수 있습니다.수렴성앞서 말한 바와 같이 이분법은 답과 유사한 값을 찾아가는 과정이기 때문에 오차가 발생하게 되는데, 이에 따라 필연적으로 ‘정확도’라는 개념이 따라옵니다.가정 f가 구간 [a0, b0]의 양 끝에서 서로 다른 부호의 함숫값을 갖는 연속함수라고 가정 [a0, b0] 내에 한 근 r이 존재. 중점 「c0 = (a0 + b0) / 2」를 근 r의 근삿값으로 하면 「┃r - c0┃ ≤ (b0 - a0) / 2」의 부등식을 얻을 수 있다. 위의 식을 n회차로 일반화하면 각 단계마다 구간의 길이가 절반으로 줄어들기 때문에 최종적으로 「┃r - cn┃ ≤ (bn - an) / 2^(n + 1)」과 같은 부등식을 도출할 수 있다.1주차 내용은 여기까지 입니다! 다음은 ‘뉴턴법’으로 돌아오겠습니다. 읽어주셔서 감사합니다!" }, { "title": "포스팅 게재 계획 변경", "url": "/posts/Study-Plan/", "categories": "공지", "tags": "장기 계획, 단기 계획", "date": "2024-09-06 13:00:38 +0900", "snippet": "막상 학기가 개강하고 나니, ‘다른 강의도 정리해두는게 좋지 않을까?’ 하는 생각이 갑자기 들었습니다… ㅎㅎ 이번 학기 학부 연구생, 기초종합설계가 정말 많은 시간을 소요할 것으로 예상이 되는지라, 시간이 남을 때 틈틈히 정리해야겠더라구요. (실제로 지금도 정신이 없습니다)그래서! 저번에 말씀 드렸던 분야 외에도 ‘AI 알고리즘, 컴퓨터 네트워크, 소프트웨어 공학’도 계속 올릴 예정입니다.많이 부족하지만, 그래도 최대한 배운 내용 그대로 올리도록 노력해보겠습니다 ㅎㅎ다음 글부터는 학기 중 과목 &amp; 쿠버네티스가 같이 올라갈 예정입니다.항상 읽어주셔서 감사합니다!!! (꾸벅)" }, { "title": "Rook-Ceph 구성 요소", "url": "/posts/Rook-Ceph-Components/", "categories": "Kubernetes, Rook-Ceph", "tags": "단기 계획", "date": "2024-09-05 16:09:21 +0900", "snippet": "이번 글에서는 저번 글에 이어 Rook-Ceph 구성요소에 대해 간략히 설명 드리겠습니다.지난 월요일부터 개강했는데, 간단한 프로젝트일 줄 알았던 기초설계 과목이 갑작스럽게 모든 기업체가 LLM 기반 생성형 AI를 토대로 서비스 개발을 뜻하시는 바람에… 지난 학기 이후로 잠시 접어두었던 딥러닝 공부를 다시 시작했습니다 ㅠㅠ이에 더해 학부 연구생 업무까지 시간이 매우 촉박해져 Rook-Ceph에 대한 자세한 설명은 최소한 2주 뒤부터 시작해야될 것 같습니다 (ㅜ^ㅜ)당장은 자세히 설명 못드리는 점, 넓은 마음으로 양해 부탁드립니다…현재 학과 서버 환경에는 다음과 같은 구성 요소들을 통해 PV를 관리 및 제공합니다.[Rook Level Component] Operator: 클러스터의 전반적인 관리 및 오케스트레이션을 담당합니다. Ceph 클러스터 생성 및 구성 스토리지 리소스 관리 클러스터 상태 모니터링 및 조정 CRDs 처리 CRDs (Custom Resource Definitions): 쿠버네티스에 Rook-Ceph 관련 사용자 정의 리소스 타입 정의 CephCluster CephBlockPool CephFileSystem CephObjectStore CephClient Toolbox: 디버깅 및 관리 도구 제공 Ceph 명령어 실행 클러스터 상태 진단 및 로그 확인 [Ceph Level Component] Cluster: 분산 스토리지 기능 제공 MON (Monitor): 클러스터 맵 관리, 클라이언트 인증 MGR (Manager): 추가 모니터링 기능, 대시보드 제공 OSD (Object Storage Device): 실제 데이터 저장 객체 MDS (Metadata Server): CephFS(Ceph FileSystem) 메타데이터 관리 CSI Proviosioner: 동적 볼륨 프로비저너 다음 Rook-Ceph 관련 글에서는 좀 더 자세한 메커니즘 설명으로 돌아오겠습니다.오늘도 읽어주셔서 감사합니다!!!" }, { "title": "Rook-Ceph 채택 배경", "url": "/posts/Why-Rook/", "categories": "Kubernetes, Rook-Ceph", "tags": "단기 계획", "date": "2024-09-04 14:15:45 +0900", "snippet": "오늘은 PV 동적 프로비저닝 관리 도구인 Rook-Ceph의 채택 사유 대해 설명해드리려고 합니다.기존에는 PV를 파드에 할당 시키기 위해 해당 파드가 존재하는 노드의 디렉토리를 직접 설정했는데, 이 디렉토리 경로를 yaml 상에선 ‘hostPath’로 명세합니다. 호스트 PC에 실재하는 디렉토리 경로를 사용하기 때문에, 초기 구현이 쉽다는 장점이 있습니다. 그러나 이러한 방식은 두 가지 문제점이 있습니다.첫 번째는 ‘보안’입니다. 호스트 PC의 루트 디렉토리 예하의 디렉토리 및 파일들에 접근하는 방식이기 때문에, 직접적으로 접근한다는 점에 있어 보안상의 문제가 우려됩니다.두 번째는 ‘작동 오류 시 특정 파드로의 올바른 PV 연결 불가’입니다. 만약 어떠한 문제가 발생하여 파드 혹은 노드에 작동 오류 발생 시, 파드의 desired value 유지를 위해 쿠버네티스가 해당 파드를 재가동합니다. 이때, 만약 다중 클러스터 환경에서 이러한 문제가 발생 했다면, 문제가 발생했던 파드에 해당 파드가 똑같이 재생성 될 것이라는 보장이 없습니다. 물론, yaml내 설정을 통해 특정 노드에 고정적으로 파드가 생성되도록 할 수도 있지만, 만약 특정 노드가 재기 불가능한 문제가 발생한다면, 해당 노드가 복구될 때 까지 해당 파드는 사용할 수 없을 수 있다는 단점이 있어 실시간으로 desired value를 유지하려는 저희의 지향하는 바와는 다소 거리가 먼 방법입니다. 이때 만약 원래 사용되던 파드가 다른 노드에 배치될 경우, 해당 파드는 기존 노드의 hostPath와 연결되도록 설정되어 있기에, 정상 작동되지 않습니다.이 뿐만 아니라 서버 관리자라면 ‘비효율적인 저장소 할당 방법’이라는 점도 있습니다. PV를 파드가 사용하기 위해선 반드시 이를 요청하기 위한 영구 저장소 요청(Persistent Volume Claim, 이하 PVC)가 적용되어야 합니다. 이때 하나의 PVC엔 단 하나의 PV만 연결되기 때문에 만약 수십 명의 학부생들에게 독자적인 공간을 제공해주려면 그 많은 파드, PV, PVC를 직접 하나하나 할당시켜주어야 합니다. 이는 최초 환경 조성, 향후 서버 관리 측면에도 상당히 불편하며, 비효율적입니다.요약하자면, 원활한 저장소 환경 관리를 위해 필요한 일들은 다음과 같습니다. 호스트 PC의 디렉토리에 직접적인 접근이 없는 저장소 할당 관리자의 원시적인 PV 할당 지양 파드 혹은 노드의 비정상적인 종료 이후에도 올바른 PV 할당1, 3번 문제를 해결하기 위해선 클러스터 전범위로 적용되는 파일 시스템이 필요합니다. 또한 각 PV에 대한 상태를 모니터링 하며, 리소스의 작동 오류에도 올바른 저장소에 연결해주는 시스템도 구축되어야 합니다.2번 문제를 해결하기 위해선 저장소가 동적으로 할당되는, 즉 동적 프로비저닝(Dynamic Proviosioning)이 필요합니다. 관리자가 새로운 파드 생성, 혹은 기존 파드 삭젫 인해 생기는 PV 관리 문제를 직접 해결하지 않고 오직 PVC의 유무 만으로 PV 생성 여부가 결정되는 시스템이 필요합니다.앞서 언급된 문제들을 해결할 수 있는 방법이 바로 ‘Rook과 Ceph을 활용하는 것’입니다.Rook은 오픈소스 클라우드 네이티브 스토리지 오케스트레이션 도구입니다. 이와 더불어 사용되는 Ceph은 오픈소스 분산 스토리지 시스템으로, Rook을 통해 Ceph을 통해 생성된 저장소 객체를 관리합니다.Rook-Ceph의 Management Flow는 하단의 이미지와 같습니다.Rook-Ceph Management Flowgraph TD A[Rook Operator 배포] --&gt; B[CRD 생성] B --&gt; C[Ceph Cluster CRD 적용] C --&gt; D[Rook이 Ceph 구성요소 배포] D --&gt; E[Ceph 클러스터 초기화] E --&gt; F[스토리지 클래스 생성] F --&gt; G[PVC 생성 및 사용] G --&gt; H[모니터링 및 관리] H --&gt; I{스케일링이 필요한가?} I --&gt;|Yes| J[CRD 업데이트] J --&gt; D I --&gt;|No| H위의 과정을 통해 Rook이 Ceph에서 정의된 가상의 리소스들을 관리하여 관리자가 요구한 상태(Desired State)를 유지하도록 도와줍니다.다음 글은 Rook-Ceph의 구성요소에 대해 설명드리겠습니다. 읽어주셔서 감사합니다!!!" }, { "title": "쿠버네티스 환경 구축 배경", "url": "/posts/Kubernetes-BackGround/", "categories": "Kubernetes", "tags": "단기 계획", "date": "2024-09-03 00:15:45 +0900", "snippet": "이번 글에서는 ‘쿠버네티스 환경 구축’을 시도하게 된 배경에 대해 이야기하려고 합니다.현재 저는 대학교 3학년 학부생입니다. 보통 이 시기가 되면 다들 팀프로젝트에 관심을 많이 보입니다. 이에 따라 자연스럽게 프로젝트 주제가 여러 분야로 나뉘게 되는데, 이 중 정말 많이 채택되는 분야가 바로 ‘딥러닝’입니다.딥러닝은 사용되는 모델의 복잡도가 다소 높거나, 데이터 양 자체가 많으면 그만큼 컴퓨팅 자원을 많이 이용하게 되는데요, 이 정도로 고성능을 요구하는 작업을 특정 회사나 단체가 아닌 개인이 하게 될 경우, 초기 비용이 정말 많이 들어갑니다… 최근 들어서는 가장 많은 비용을 차지하는 부품인 GPU에 이전보단 비교적 적은 수준의 금액을 투자해도 GPU 환경을 구축할 수 있게 되었지만, 그래도 일반 소비자층이 소비하기엔 큰 비용이 요구되다보니, 개인이 이러한 환경을 구축하기에는 다소 무리인 감이 있습니다.이에 따라 학과 내에선 실습 환경을 대체로 ‘Colab(이하 코랩)’을 사용하고 있습니다.코랩은 jupyter notebook 환경을 기본적으로 제공하면서 여러 python 라이브러리를 사용할 수 있도록 하는 서비스입니다. 이런 여러 라이브러리들엔 머신러닝, 딥러닝과 관련된 것들 또한 포함되어 있습니다. 또한 구글 계정만 있으면 누구나 사용할 수 있다는 접근의 용이함 덕분에 AI 관련 분야 입문자 뿐만 아니라 전문적인 종사자들 또한 코랩 서비스를 다수 사용하는 경향이 있습니다.그러나, 무료로 제공되는 서비스는 컴퓨팅 자원, 사용 시간의 제약으로 인해 복잡도가 큰 모델을 이용한 학습, 데이터 단위가 TB 이상인 자료들을 이용한 학습을 진행하기에는 턱없이 부족합니다. 이에 따라 코랩에서는 부분 유료화를 통해 컴퓨팅 자원 가용량에 따라 월 $9.99와 월 $49.99로 가용 리소스에 차별을 두어 운영하고 있는데, 누적되면 절대 적지 않은 금액을 지불해야 한다는 단점이 있습니다. 특히 대학생들의 경우 이 비용을 스스로 지불한다면 2년 정도 사용한다고 예상해도 약 $120이 소비되기 때문에 절대 좌시할 수 없는 비용임에는 틀림없습니다.따라서 저를 비롯한 학부 연구생들은 학과에 제공 가능한 분산 컴퓨팅 시스템을 쿠버네티스를 통해 도입하기로 했습니다. 단일 컴퓨터를 사용하여 학습을 진행하는 환경은 각각의 단일 리소스들에 모든 부하가 쏠리게 되는 반면, 쿠버네티스 기반의 분산 컴퓨팅을 이용하면, n개의 컴퓨터에 m개의 리소스들이 각각 일을 분산하여 할당받기 때문에 비교적 낮은 성능의 컴퓨터들을 한 집합에 묶어 고성능의 컴퓨터가 가진 성능과 비슷하거나 상회하는 수준의 수행 능력을 기대할 수 있습니다.다음 글에서는 ‘쿠버네티스로 분산 컴퓨팅을 구현하기 위한 방법 및 학과 시스템의 설계’로 돌아오겠습니다. 읽어주셔서 감사합니다!!!" }, { "title": "첫 블로그 게재 및 추후 계획", "url": "/posts/first-post/", "categories": "공지", "tags": "단기 계획", "date": "2024-08-31 00:52:45 +0900", "snippet": "그간 공부한 내용에 있어서 시간이 지나면 기억에서 희미해지는걸 막고자,드디어 기술 블로그를 개설하게 되었습니다. 앞으로 공부한 바에 대해 작성하면서다른 사람들에게도 도움이 되려고 해요 d_(^.^)_b앞으로의 계획은 다음과 같습니다![장기 계획]쿠버네티스, 데이터베이스, 컴퓨터구조, 운영체제, 서버 관련[단기 계획]8월 31일 ~ 9월 중순: 쿠버네티스 전반에 대한 설명9월 중순 ~ 10월 2주차: 학기중 데이터베이스 관련 내용 정리다음 단기 계획은 위의 내용을 모두 시행에 옮기고 재공지하도록 하겠습니다 ㅎㅎ" } ]
